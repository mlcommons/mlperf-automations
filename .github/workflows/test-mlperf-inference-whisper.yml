name: MLPerf inference Whisper
on:
    workflow_dispatch:
  

jobs:
  run_whisper_reference:
    if: github.repository_owner == 'mlcommons'
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: [ "3.12" ]
        backend: [ "pytorch" ]
        precision: [ "float16" ]
    steps:
    - name: Load RunPod secret
      id: op-load-secret
      uses: 1password/load-secrets-action@v2
      with:
          export-env: false
      env:
          OP_SERVICE_ACCOUNT_TOKEN: ${{ secrets.OP_SERVICE_ACCOUNT_TOKEN }}
          RUNPOD_API_KEY: op://7basd2jirojjckncf6qnq3azai/3a3onp754cpzbw3xeq5noeb26a/credential

    - name: Install tools
      run: sudo apt-get update && sudo apt-get install -y jq

    
    - name: Create Pod (custom image, 200GB volume)
      id: create_pod
      env:
        RUNPOD_API_KEY: ${{ steps.op-load-secret.outputs.RUNPOD_API_KEY }}
        GPU_TYPE: NVIDIA RTX A4500
      run: |
        q='mutation($i:PodFindAndDeployOnDemandInput!){
          podFindAndDeployOnDemand(input:$i){ id status imageName }
        }'
        vars=$(jq -n --arg img "nvcr.io/nvidia/pytorch:24.08-py3" \
                     --arg gpu "$GPU_TYPE" \
                     --argjson vol 200 \
                     --argjson cdisk 40 \
                     --arg reg "${REG_AUTH_ID:-}" \
              '{i:{cloudType:"SECURE", gpuTypeId:$gpu, gpuCount:1,
                   volumeInGb:$vol, containerDiskInGb:$cdisk,
                   imageName:$img, dockerArgs:"", ports:"",
                   volumeMountPath:"/workspace"}, env: [{key:"MLC_REPOS", value: "/workspace/mlc/repos"}]} + ( $reg|length>0 ? {i:{containerRegistryAuthId:$reg}} : {} )')
        resp=$(curl -sS -H "Authorization: Bearer $RUNPOD_API_KEY" -H "Content-Type: application/json" \
               -d "$(jq -n --arg q "$q" --argjson v "$vars" '{query:$q, variables:$v}')" \
               https://api.runpod.io/graphql)
        echo "$resp" | jq .
        echo "pod_id=$(echo "$resp" | jq -r '.data.podFindAndDeployOnDemand.id')" >> "$GITHUB_OUTPUT"

    - name: Wait until RUNNING
      id: wait
      env:
        RUNPOD_API_KEY: ${{ env.RUNPOD_API_KEY }}
        POD_ID: ${{ steps.create_pod.outputs.pod_id }}
      run: |
        q='query($id:ID!){ pod(id:$id){ id status } }'
        for i in {1..120}; do
          resp=$(curl -sS -H "Authorization: Bearer $RUNPOD_API_KEY" -H "Content-Type: application/json" \
            -d "$(jq -n --arg q "$q" --arg id "$POD_ID" '{query:$q,"variables":{"id":$id}}')" \
            https://api.runpod.io/graphql)
          s=$(echo "$resp" | jq -r '.data.pod.status')
          echo "status=$s"
          [ "$s" = "RUNNING" ] && exit 0
          sleep 10
        done
        echo "Pod did not reach RUNNING"; exit 1    

    - name: Prepare environment inside Pod (apt + pip)
      env:
        RUNPOD_API_KEY: ${{ env.RUNPOD_API_KEY }}
        POD_ID: ${{ steps.create_pod.outputs.pod_id }}
      run: |
        execq='mutation($i:PodExecInput!){ podExec(input:$i){ id output } }'
        runexec(){ curl -sS -H "Authorization: Bearer $RUNPOD_API_KEY" -H "Content-Type: application/json" \
          -d "$(jq -n --arg q "$execq" --arg id "$POD_ID" --arg cmd "$1" '{query:$q,variables:{i:{podId:$id,command:$cmd}}}')" \
          https://api.runpod.io/graphql | jq -r '.data.podExec.output'; }
        runexec "apt-get update && apt-get install -y python3 python3-venv python3-pip wget unzip git curl tmux"
        runexec "pip3 install --user mlc-script"
        runexec "mkdir -p /workspace/logs && echo 'export PATH=\$HOME/.local/bin:\$PATH' >> ~/.bashrc"

    - name: Launch Whisper benchmark (detached via tmux)
      env:
        RUNPOD_API_KEY: ${{ env.RUNPOD_API_KEY }}
        POD_ID: ${{ steps.create_pod.outputs.pod_id }}
        BACKEND: ${{ matrix.backend }}
        PRECISION: ${{ matrix.precision }}
      run: |
        execq='mutation($i:PodExecInput!){ podExec(input:$i){ id output } }'
        START=$(cat <<EOF bash -lc "tmux new -d -s bench-whisper bash -lc \"source ~/.bashrc && mlcr run-mlperf,inference,_full,_r5.1-dev --model=whisper --implementation=reference --framework=vllm --category=datacenter --scenario=Offline --execution_mode=valid --device=cuda --quiet |& tee -a /workspace/logs/whisper.log; echo DONE > /workspace/logs/whisper.done\""EOF)
        curl -sS -H "Authorization: Bearer $RUNPOD_API_KEY" -H "Content-Type: application/json" \
        -d "$(jq -n --arg q "$execq" --arg id "$POD_ID" --arg cmd "$START" '{query:$q,variables:{i:{podId:$id,command:$cmd}}}')" \
        https://api.runpod.io/graphql | jq -r '.data.podExec.output'
    
    - name: Show initial log tail
      env:
        RUNPOD_API_KEY: ${{ env.RUNPOD_API_KEY }}
        POD_ID: ${{ steps.create_pod.outputs.pod_id }}
      run: |
        execq='mutation($i:PodExecInput!){ podExec(input:$i){ id output } }'
        for i in {1..3}; do
            out=$(curl -sS -H "Authorization: Bearer $RUNPOD_API_KEY" -H "Content-Type: application/json" \
              -d "$(jq -n --arg q "$execq" --arg id "$POD_ID" --arg cmd 'tail -n 50 /workspace/logs/whisper.log || true' '{query:$q,variables:{i:{podId:$id,command:$cmd}}}')" \
              https://api.runpod.io/graphql | jq -r '.data.podExec.output')
            echo "$out"
            sleep 20
        done

    - name: Test MLPerf Inference SDXL Reference
      run: |
        source gh_action/bin/deactivate || python3 -m venv gh_action
        source gh_action/bin/activate
        export MLC_REPOS=$HOME/GH_MLC
        python3 -m pip install mlc-scripts
        mlc pull repo
        mlcr run-mlperf,inference,_submission,_short --submitter="MLCommons" --pull_changes=yes --pull_inference_changes=yes --docker --model=sdxl --backend=${{ matrix.backend }} --device=cuda --scenario=Offline --test_query_count=1 --precision=${{ matrix.precision }}  --quiet --docker_it=no --docker_mlc_repo=gateoverflow@mlperf-automations --adr.compiler.tags=gcc --hw_name=gh_action --docker_dt=yes  --results_dir=$HOME/gh_action_results --submission_dir=$HOME/gh_action_submissions  --env.MLC_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes --clean
        mlcr push,github,mlperf,inference,submission --repo_url=https://github.com/mlcommons/mlperf_inference_test_submissions_v5.0 --repo_branch=auto-update --commit_message="Results from self hosted Github actions - NVIDIARTX4090" --quiet --submission_dir=$HOME/gh_action_submissions
