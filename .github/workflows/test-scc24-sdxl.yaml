name: MLPerf inference SDXL (SCC)

on:
  schedule:
    - cron: "34 19 * * *"

jobs:
  build_reference:
    if: github.repository_owner == 'gateoverflow'
    runs-on: [ self-hosted, linux, x64, GO-spr ]
    env:
      MLC_DOCKER_REPO: mlcommons@mlperf-automations
      MLC_DOCKER_REPO_BRANCH: dev
    strategy:
      fail-fast: false
      matrix:
        python-version: [ "3.12" ]
        backend: [ "pytorch" ]
        precision: [ "float16" ]
        device: [ "cuda" ]
    steps:
    - name: Test MLPerf Inference reference SDXL SCC 
      run: |
        if [ -f "gh_action/bin/deactivate" ]; then source gh_action/bin/deactivate; fi
        python3 -m venv gh_action
        source gh_action/bin/activate
        export MLC_REPOS=$HOME/GH_MLC
        pip install --upgrade mlcflow
        pip install tabulate
        mlc pull repo
        mlcr --tags=run-mlperf,inference,_find-performance,_r4.1-dev,_short,_scc24-base --pull_changes=yes --pull_inference_changes=yes --model=sdxl --implementation=reference --backend=${{ matrix.backend }} --category=datacenter --scenario=Offline --execution_mode=test --device=${{ matrix.device }} --precision=${{ matrix.precision }} --docker --docker_it=no --docker_mlc_repo=$MLC_DOCKER_REPO --docker_mlc_repo_branch=$MLC_DOCKER_REPO_BRANCH --docker_dt=yes --quiet --results_dir=$HOME/scc_gh_action_results --submission_dir=$HOME/scc_gh_action_submissions --env.MLC_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes --clean
        mlcr --tags=run-mlperf,inference,_r4.1-dev,_short,_scc24-base --model=sdxl --implementation=reference --backend=${{ matrix.backend }} --category=datacenter --scenario=Offline --execution_mode=test --device=${{ matrix.device }} --precision=${{ matrix.precision }} --docker --docker_it=no --docker_mlc_repo=$MLC_DOCKER_REPO --docker_mlc_repo_branch=$MLC_DOCKER_REPO_BRANCH --docker_dt=yes --quiet --results_dir=$HOME/scc_gh_action_results --submission_dir=$HOME/scc_gh_action_submissions --env.MLC_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes --clean
        mlcr --tags=generate,inference,submission --clean  --run-checker --tar=yes --env.MLC_TAR_OUTFILE=submission.tar.gz --division=open --category=datacenter --run_style=test --adr.submission-checker.tags=_short-run --quiet --submitter=MLCommons --submission_dir=$HOME/scc_gh_action_submissions --results_dir=$HOME/scc_gh_action_results/test_results
        mlcr --tags=push,github,mlperf,inference,submission --repo_url=https://github.com/gateoverflow/cm4mlperf-inference --repo_branch=mlperf-inference-results-scc24 --commit_message="Results from self hosted Github actions - NVIDIARTX4090" --quiet --submission_dir=$HOME/scc_gh_action_submissions
        
  build_nvidia:
      if: github.repository_owner == 'gateoverflow'
      runs-on: [ self-hosted, linux, x64, GO-spr]
      env:
        MLC_DOCKER_REPO: mlcommons@mlperf-automations
        MLC_DOCKER_REPO_BRANCH: dev
      strategy:
        fail-fast: false
        matrix:
          python-version: [ "3.12" ]
          backend: [ "tensorrt" ]
          precision: [ "float16" ]
          implementation: [ "nvidia" ]
      steps:
      - name: Test MLPerf Inference NVIDIA SDXL SCC
        run: |
          if [ -f "gh_action/bin/deactivate" ]; then source gh_action/bin/deactivate; fi
          python3 -m venv gh_action
          source gh_action/bin/activate
          export MLC_REPOS=$HOME/GH_MLC
          pip install --upgrade mlcflow
          pip install tabulate
          mlc pull repo
          mlcr --tags=run-mlperf,inference,_find-performance,_r4.1-dev,_short,_scc24-base --pull_changes=yes --model=sdxl --implementation=nvidia --backend=${{ matrix.backend }} --category=datacenter --scenario=Offline --execution_mode=test --device=${{ matrix.device }} --precision=${{ matrix.precision }} --docker --docker_it=no --docker_mlc_repo=$MLC_DOCKER_REPO --docker_mlc_repo_branch=$MLC_DOCKER_REPO_BRANCH --docker_dt=yes --pull_changes --quiet --results_dir=$HOME/scc_gh_action_results --submission_dir=$HOME/scc_gh_action_submissions --env.MLC_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes --hw_name=go-spr --custom_system_nvidia=yes --clean
          mlcr --tags=run-mlperf,inference,_r4.1-dev,_short,_scc24-base --model=sdxl --implementation=nvidia --backend=${{ matrix.backend }} --category=datacenter --scenario=Offline --execution_mode=test --device=${{ matrix.device }} --precision=${{ matrix.precision }} --docker --docker_it=no --docker_mlc_repo=$MLC_DOCKER_REPO --docker_mlc_repo_branch=$MLC_DOCKER_REPO_BRANCH --docker_dt=yes --quiet --results_dir=$HOME/scc_gh_action_results --submission_dir=$HOME/scc_gh_action_submissions --env.MLC_MLPERF_MODEL_SDXL_DOWNLOAD_TO_HOST=yes --clean
          mlcr --tags=generate,inference,submission --clean --run-checker --tar=yes --env.MLC_TAR_OUTFILE=submission.tar.gz --division=open --category=datacenter --run_style=test --adr.submission-checker.tags=_short-run --quiet --submitter=MLCommons --submission_dir=$HOME/scc_gh_action_submissions --results_dir=$HOME/scc_gh_action_results/test_results
          mlcr --tags=push,github,mlperf,inference,submission --repo_url=https://github.com/gateoverflow/cm4mlperf-inference --repo_branch=mlperf-inference-results-scc24 --commit_message="Results from self hosted Github actions - NVIDIARTX4090" --quiet --submission_dir=$HOME/scc_gh_action_submissions
