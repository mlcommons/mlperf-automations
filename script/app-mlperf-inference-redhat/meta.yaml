# Identification of this MLC script
alias: app-mlperf-inference-redhat
uid: 82c9bb3c222447ca
cache: false

automation_alias: script
automation_uid: 5b4e0237da074764

category: "Modular MLPerf benchmarks"


# User-friendly tags to find this MLC script
tags:
  - reproduce
  - mlcommons
  - mlperf
  - inference
  - harness
  - redhat-harness
  - redhat

# Default environment
default_env:
  MLC_MLPERF_LOADGEN_SCENARIO: Offline
  MLC_MLPERF_LOADGEN_MODE: performance
  MLC_SKIP_PREPROCESS_DATASET: 'no'
  MLC_SKIP_MODEL_DOWNLOAD: 'no'
  MLC_MLPERF_SUT_NAME_IMPLEMENTATION_PREFIX: redhat_harness
  MLC_MLPERF_SKIP_RUN: 'no'

env:
  MLC_CALL_MLPERF_RUNNER: 'no'

# Map script inputs to environment variables
input_mapping:
  count: MLC_MLPERF_LOADGEN_QUERY_COUNT
  max_batchsize: MLC_MLPERF_LOADGEN_MAX_BATCHSIZE
  mlperf_conf: MLC_MLPERF_CONF
  mode: MLC_MLPERF_LOADGEN_MODE
  output_dir: MLC_MLPERF_OUTPUT_DIR
  performance_sample_count: MLC_MLPERF_LOADGEN_PERFORMANCE_SAMPLE_COUNT
  scenario: MLC_MLPERF_LOADGEN_SCENARIO
  user_conf: MLC_MLPERF_USER_CONF
  skip_preprocess: MLC_SKIP_PREPROCESS_DATASET
  skip_preprocessing: MLC_SKIP_PREPROCESS_DATASET
  target_qps: MLC_MLPERF_LOADGEN_TARGET_QPS
  offline_target_qps: MLC_MLPERF_LOADGEN_OFFLINE_TARGET_QPS
  server_target_qps: MLC_MLPERF_LOADGEN_SERVER_TARGET_QPS
  target_latency: MLC_MLPERF_LOADGEN_TARGET_LATENCY
  singlestream_target_latency: MLC_MLPERF_LOADGEN_SINGLESTREAM_TARGET_LATENCY
  multistream_target_latency: MLC_MLPERF_LOADGEN_MULTISTREAM_TARGET_LATENCY
  performance_sample_count: MLC_MLPERF_LOADGEN_PERFORMANCE_SAMPLE_COUNT
  rerun: MLC_RERUN
  results_repo: MLC_MLPERF_INFERENCE_RESULTS_REPO

new_state_keys:
  - mlperf-inference-implementation
  - MLC_SUT_*

# Env keys which are exposed to higher level scripts
new_env_keys:
  - MLC_MLPERF_*
  - MLC_DATASET_*
  - MLC_HW_NAME
  - MLC_ML_MODEL_*
  - MLC_MAX_EXAMPLES
  - MLC_IMAGENET_ACCURACY_DTYPE
  - MLC_SQUAD_ACCURACY_DTYPE


# Dependencies on other MLC scripts

deps:

  # Detect host OS features
  - tags: detect,os

  # Detect host CPU features
  - tags: detect,cpu

  # Install system dependencies on a given host
  - tags: get,sys-utils-mlc


  ########################################################################
  # Install MLPerf inference dependencies

  # Download MLPerf inference source
  - tags: get,mlcommons,inference,src
    names:
    - inference-src

  # Download MLPerf inference loadgen
  - tags: get,mlcommons,inference,loadgen,_wg-inference
    names:
    - inference-loadgen

  # Creates user conf for given SUT
  - tags: generate,user-conf,mlperf,inference,_wg-inference
    names:
    - user-conf-generator

  # Get MLPerf logging library
  - tags: get,generic-python-lib,_mlperf_logging
    names:
    - mlperf-logging

  - tags: get,git,repo
    names:
      - inference-results
      - inference-code
    update_tags_from_env_with_prefix:
      _repo.:
        - MLC_MLPERF_INFERENCE_RESULTS_REPO
    env:
      MLC_GIT_CHECKOUT_PATH_ENV_NAME: MLC_MLPERF_INFERENCE_IMPLEMENTATION_REPO
    extra_cache_tags: results,repo,mlperf

# Post dependencies to run this app including for power measurement
post_deps:

  - names:
    - runner
    - mlperf-runner
    skip_if_env:
      MLC_MLPERF_SKIP_RUN:
        - 'yes'
        - yes
    tags: benchmark-mlperf

  - tags: save,mlperf,inference,state
    names:
      - save-mlperf-inference-state

# Variations to customize dependencies
variations:
  # Target devices
  cpu:
    group: device
    default: true
    env:
      MLC_MLPERF_DEVICE: cpu
  cuda:
    group: device
    env:
      MLC_MLPERF_DEVICE: gpu
      MLC_MLPERF_DEVICE_LIB_NAMESPEC: cudart

  openshift:
    group: backend
    default: true
    env:
      MLC_MLPERF_BACKEND: openshift

  pytorch:
    group: backend
    env:
      MLC_MLPERF_BACKEND: pytorch

  pytorch,cuda:
    deps:
      - tags: get,generic-python-lib,_torch_cuda

  pytorch,cpu:
    deps:
      - tags: get,generic-python-lib,_torch

  bs.#:
    group: batch-size

  
  # Reference MLPerf models
  resnet50:
    group: model
    default: true
    env:
      MLC_MODEL: resnet50

  retinanet:
    group: model
    base:
      - bs.1
    env:
      MLC_MODEL: retinanet

  bert_:
    {}

  bert-99:
    group: model
    base:
    - bert_
    env:
      MLC_MODEL: bert-99
      MLC_SQUAD_ACCURACY_DTYPE: float32

  bert-99.9:
    group: model
    base:
    - bert_
    env:
      MLC_MODEL: bert-99.9

  bert_:
    {}

  bert-99:
    group: model
    base:
    - bert_
    env:
      MLC_MODEL: bert-99
      MLC_SQUAD_ACCURACY_DTYPE: float32

  bert-99.9:
    group: model
    base:
    - bert_
    env:
      MLC_MODEL: bert-99.9

  gptj_:
    deps:
      - tags: get,ml-model,gptj
        names:
         - gptj-model
      - tags: get,dataset,cnndm,_validation

  gptj-99:
    group: model
    base:
    - gptj_
    env:
      MLC_MODEL: gptj-99
      MLC_SQUAD_ACCURACY_DTYPE: float32

  gptj-99.9:
    group: model
    base:
    - gptj_
    env:
      MLC_MODEL: gptj-99.9

  llama2-70b_:
    deps:
      - tags: get,dataset,openorca,language-processing,original,_redhat
        env:
          MLC_MLPERF_IMPLEMENTATION: redhat
    env:
        MLC_VLLM_SERVER_MODEL_NAME: NousResearch/Meta-Llama-3-8B-Instruct # assigned just for testing purpose

  llama2-70b-99:
    group: model
    base:
    - llama2-70b_
    env:
      MLC_MODEL: llama2-70b-99

  llama2-70b-99.9:
    group: model
    base:
    - llama2-70b_
    env:
      MLC_MODEL: llama2-70b-99.9

  singlestream:
    group: loadgen-scenario
    env:
      MLC_MLPERF_LOADGEN_SCENARIO: SingleStream

  singlestream,resnet50:
    default_variations:
      batch-size: bs.1

  singlestream,retinanet:
    default_variations:
      batch-size: bs.1

  multistream:
    group: loadgen-scenario
    env:
      MLC_MLPERF_LOADGEN_SCENARIO: MultiStream

  offline:
    group: loadgen-scenario
    env:
      MLC_MLPERF_LOADGEN_SCENARIO: Offline

  server:
    group: loadgen-scenario
    env:
      MLC_MLPERF_LOADGEN_SCENARIO: Server

  uint8:
    group: precision
  fp16:
    group: precision
  fp32:
    group: precision

  r4.1-dev_default:
    group: version
    default: true
    env:
      MLC_MLPERF_INFERENCE_RESULTS_REPO: https://github.com/mlcommons/inference_results_v4.0

docker:
  real_run: False
