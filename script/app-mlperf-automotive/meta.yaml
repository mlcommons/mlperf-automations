alias: app-mlperf-automotive
uid: f7488ce376484fd2

automation_alias: script
automation_uid: 5b4e0237da074764

category: "Modular MLPerf inference benchmark pipeline for ABTF model"


# User-friendly tags to find this CM script
tags:
- app
- app-mlperf-inference
- mlperf-inference
- abtf-inference

predeps: no

# Default environment
default_env:
  MLC_MLPERF_LOADGEN_MODE: accuracy
  MLC_MLPERF_LOADGEN_SCENARIO: Offline
  MLC_OUTPUT_FOLDER_NAME: test_results
  MLC_MLPERF_RUN_STYLE: test
  MLC_TEST_QUERY_COUNT: '10'
  MLC_MLPERF_QUANTIZATION: off
  MLC_MLPERF_SUT_NAME_IMPLEMENTATION_PREFIX: reference
  MLC_MLPERF_SUT_NAME_RUN_CONFIG_SUFFIX: ''


# Map script inputs to environment variables
input_mapping:
  device: MLC_MLPERF_DEVICE
  count: MLC_MLPERF_LOADGEN_QUERY_COUNT
  docker: MLC_RUN_DOCKER_CONTAINER
  hw_name: MLC_HW_NAME
  imagenet_path: IMAGENET_PATH
  max_batchsize: MLC_MLPERF_LOADGEN_MAX_BATCHSIZE
  mode: MLC_MLPERF_LOADGEN_MODE
  num_threads: MLC_NUM_THREADS
  threads: MLC_NUM_THREADS
  dataset: MLC_MLPERF_VISION_DATASET_OPTION
  model: MLC_MLPERF_CUSTOM_MODEL_PATH
  output_dir: OUTPUT_BASE_DIR
  power: MLC_MLPERF_POWER
  power_server: MLC_MLPERF_POWER_SERVER_ADDRESS
  ntp_server: MLC_MLPERF_POWER_NTP_SERVER
  max_amps: MLC_MLPERF_POWER_MAX_AMPS
  max_volts: MLC_MLPERF_POWER_MAX_VOLTS
  regenerate_files: MLC_REGENERATE_MEASURE_FILES
  rerun: MLC_RERUN
  scenario: MLC_MLPERF_LOADGEN_SCENARIO
  test_query_count: MLC_TEST_QUERY_COUNT
  clean: MLC_MLPERF_CLEAN_SUBMISSION_DIR
  dataset_args: MLC_MLPERF_EXTRA_DATASET_ARGS
  target_qps: MLC_MLPERF_LOADGEN_TARGET_QPS
  target_latency: MLC_MLPERF_LOADGEN_TARGET_LATENCY
  offline_target_qps: MLC_MLPERF_LOADGEN_OFFLINE_TARGET_QPS
  server_target_qps: MLC_MLPERF_LOADGEN_SERVER_TARGET_QPS
  singlestream_target_latency: MLC_MLPERF_LOADGEN_SINGLESTREAM_TARGET_LATENCY
  multistream_target_latency: MLC_MLPERF_LOADGEN_MULTISTREAM_TARGET_LATENCY
  output: MLC_MLPERF_OUTPUT_DIR

# Env keys which are exposed to higher level scripts
new_env_keys:
  - MLC_MLPERF_*
  - MLC_OUTPUT_PREDICTIONS_PATH

new_state_keys:
  - mlc-mlperf-inference-results*

# Dependencies on other CM scripts
deps:

  # Detect host OS features
  - tags: detect,os

  # Detect host CPU features
  - tags: detect,cpu

  # Install system dependencies on a given host
  - tags: get,sys-utils-cm

  # Detect/install python
  - tags: get,python
    names:
    - python
    - python3

  # Use cmind inside CM scripts
  - tags: get,generic-python-lib,_package.cmind

  - tags: get,mlperf,inference,utils


docker:
  mlc_repo: gateoverflow@cm4mlops
  use_host_group_id: True
  use_host_user_id: True
  real_run: false
  interactive: True
  mlc_repos: 'cm pull repo mlcommons@cm4abtf --checkout=poc'
  deps:
    - tags: get,abtf,scratch,space
  mounts:
    - "${{ MLC_ABTF_SCRATCH_PATH_DATASETS }}:${{ MLC_ABTF_SCRATCH_PATH_DATASETS }}"


# Variations to customize dependencies
variations:

  # Implementation
  mlcommons-python:
    group: implementation
    default: true
    env:
      MLC_MLPERF_PYTHON: 'yes'
      MLC_MLPERF_IMPLEMENTATION: reference
    prehook_deps:
      - names:
         - python-reference-abtf-inference
         - abtf-inference-implementation
        tags: run-mlperf-inference,demo,abtf-model
        skip_if_env:
          MLC_SKIP_RUN:
            - yes


  # Execution modes
  fast:
    group: execution-mode
    env:
      MLC_FAST_FACTOR: '5'
      MLC_OUTPUT_FOLDER_NAME: fast_results
      MLC_MLPERF_RUN_STYLE: fast

  test:
    group: execution-mode
    default: true
    env:
      MLC_OUTPUT_FOLDER_NAME: test_results
      MLC_MLPERF_RUN_STYLE: test

  valid:
    group: execution-mode
    env:
      MLC_OUTPUT_FOLDER_NAME: valid_results
      MLC_MLPERF_RUN_STYLE: valid


  # ML engine
  onnxruntime:
    group: framework
    env:
      MLC_MLPERF_BACKEND: onnxruntime
    add_deps_recursive:
      abtf-inference-implementation:
        tags: _onnxruntime


  onnxruntime,cpu:
    env:
      MLC_MLPERF_BACKEND_VERSION: <<<MLC_ONNXRUNTIME_VERSION>>>

  onnxruntime,cuda:
    env:
      MLC_MLPERF_BACKEND_VERSION: <<<MLC_ONNXRUNTIME_GPU_VERSION>>>
      ONNXRUNTIME_PREFERRED_EXECUTION_PROVIDER: "CUDAExecutionProvider"


  pytorch:
    group: framework
    default: true
    env:
      MLC_MLPERF_BACKEND: pytorch
      MLC_MLPERF_BACKEND_VERSION: <<<MLC_TORCH_VERSION>>>
    add_deps_recursive:
      abtf-inference-implementation:
        tags: _pytorch


  abtf-demo-model:
    env:
      MLC_MODEL: retinanet
    group: models
    add_deps_recursive:
      abtf-inference-implementation:
        tags: _abtf-demo-model

  abtf-poc-model:
    env:
      MLC_MODEL: retinanet
    default: true
    group: models
    add_deps_recursive:
      abtf-inference-implementation:
        tags: _abtf-poc-model
    docker:
      deps:
        - tags: get,dataset,raw,mlcommons-cognata,_abtf-poc
          names:
          - raw-dataset-mlcommons-cognata
          enable_if_env:
            MLC_DATASET_MLCOMMONS_COGNATA_DOWNLOAD_IN_HOST:
              - yes

      mounts:
        - "${{ MLC_DATASET_MLCOMMONS_COGNATA_PATH }}:${{ MLC_DATASET_MLCOMMONS_COGNATA_PATH }}"


  # Target devices
  cpu:
    group: device
    default: true
    env:
      MLC_MLPERF_DEVICE: cpu
      CUDA_VISIBLE_DEVICES: ''
      USE_CUDA: no
      USE_GPU: no
    add_deps_recursive:
      abtf-inference-implementation:
        tags: _cpu

  cuda:
    group: device
    env:
      MLC_MLPERF_DEVICE: gpu
      USE_CUDA: yes
      USE_GPU: yes
    add_deps_recursive:
      abtf-inference-implementation:
        tags: _cuda
    docker:
      all_gpus: 'yes'
      base_image: nvcr.io/nvidia/pytorch:24.03-py3



  # Loadgen scenarios
  offline:
    env:
      MLC_MLPERF_LOADGEN_SCENARIO: Offline
    add_deps_recursive:
      abtf-inference-implementation:
        tags: _offline
  multistream:
    env:
      MLC_MLPERF_LOADGEN_SCENARIO: MultiStream
    add_deps_recursive:
      abtf-inference-implementation:
        tags: _multistream
  singlestream:
    group: loadgen-scenario
    default: true
    env:
      MLC_MLPERF_LOADGEN_SCENARIO: SingleStream
    add_deps_recursive:
      abtf-inference-implementation:
        tags: _singlestream
  server:
    env:
      MLC_MLPERF_LOADGEN_SCENARIO: Server
    add_deps_recursive:
      abtf-inference-implementation:
        tags: _server

  mvp-demo:
    env:
      MLC_ABTF_MVP_DEMO: yes
      MLC_MLPERF_VISION_DATASET_OPTION: cognata-8mp-pt
      MLC_ABTF_ML_MODEL_CONFIG: baseline_8MP_ss_scales_all
      MLC_ABTF_NUM_CLASSES: 15
      MLC_DATASET_MLCOMMONS_COGNATA_SERIAL_NUMBERS: 10002_Urban_Clear_Morning
      MLC_DATASET_MLCOMMONS_COGNATA_GROUP_NAMES: Cognata_Camera_01_8M
      MLC_ABTF_ML_MODEL_TRAINING_FORCE_COGNATA_LABELS: 'yes'
      MLC_ABTF_ML_MODEL_SKIP_WARMUP: 'yes'

  poc-demo:
    env:
      MLC_ABTF_POC_DEMO: yes
      MLC_MLPERF_VISION_DATASET_OPTION: cognata-8mp-pt
      MLC_ABTF_ML_MODEL_CONFIG: baseline_8MP_ss_scales_fm1_5x5_all
      MLC_ABTF_NUM_CLASSES: 15
      MLC_DATASET_MLCOMMONS_COGNATA_SERIAL_NUMBERS: 10002_Urban_Clear_Morning
      MLC_DATASET_MLCOMMONS_COGNATA_GROUP_NAMES: Cognata_Camera_01_8M
      MLC_ABTF_ML_MODEL_TRAINING_FORCE_COGNATA_LABELS: 'yes'
      MLC_ABTF_ML_MODEL_SKIP_WARMUP: 'yes'
